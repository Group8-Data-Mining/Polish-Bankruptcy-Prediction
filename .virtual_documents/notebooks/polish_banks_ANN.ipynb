








# === Standard Libraries ===
import os
import numpy as np
import pandas as pd

# === Visualization ===
import matplotlib.pyplot as plt
import seaborn as sns

# === Data Processing ===
from scipy.io import arff
from sklearn.utils import shuffle
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV, RandomizedSearchCV,KFold
from sklearn.utils import resample

# === Classifiers ===
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
import xgboost as xgb

# === Metrics ===
from sklearn.metrics import (
    classification_report,
    accuracy_score,
    precision_score,
    recall_score,
    f1_score,
    confusion_matrix,
    matthews_corrcoef,
    roc_auc_score,
    average_precision_score,
    cohen_kappa_score,
    precision_recall_curve,
    precision_recall_fscore_support
)

# === Imbalanced Learning ===
from imblearn.over_sampling import SMOTE
from imblearn.pipeline import Pipeline

# === Deep Learning ===
import tensorflow as tf
import keras
from keras import layers, regularizers, backend as K
from keras.models import Model, Sequential
from keras.regularizers import l2
from sklearn.utils.class_weight import compute_class_weight
from tensorflow.keras.models import load_model
from tensorflow.keras.layers import (
    Conv1D, Dense, MaxPooling1D, Input, Flatten,
    LSTM, Dropout, Bidirectional, LeakyReLU, SimpleRNN
)
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.utils import plot_model
from tensorflow.keras.losses import BinaryCrossentropy
from tensorflow.keras.callbacks import EarlyStopping

# === Hyperparameter Tuning ===
import keras_tuner as kt
from keras_tuner import HyperParameters

# === Dataset Fetching ===
# from ucimlrepo import fetch_ucirepo








# reading the data file and then sepperating the class feature
data = pd.read_csv('/content/drive/Shareddrives/CS5831 Data Mining Spring 2025/Data/data.csv')
X = data.drop(columns=['class'])
y = data['class']





# Visualizing missing data
sns.heatmap(X.isnull(), cbar=True)
plt.title('Missing Data Heatmap')
plt.xlabel('Predictors')
plt.ylabel('Samples')
plt.xticks(rotation=45, ha='right')
plt.show()





# Assuming X and y are DataFrames or Series
full_dataset = pd.concat([X, y], axis=1)

# Count the number of missing values
missing_counts = full_dataset.isnull().sum()
# Sort by highest missing values and get top results
top_missing = missing_counts.sort_values(ascending=False)
top_missing.head(20)
missing_counts = full_dataset.isnull().sum()

# Select columns to drop based on missing over 1000 values
columns_to_drop = missing_counts[missing_counts > 1000].index
# Delete missing columns
combined_dataset = full_dataset.drop(columns=columns_to_drop)
print(100 * missing_counts/len(full_dataset))
print(100 * top_missing/len(full_dataset) )
print(columns_to_drop)
combined_dataset = combined_dataset.dropna()
print(f"Removed rows with missing values. New dataset shape: {combined_dataset.shape}")





###I ran this code here to check to make sure removing certain columns and rows wouldnt affect class counts too much
#It evenly removed from both 0 and 1
# class_counts = copy_dataset['class'].value_counts()
# class_counts
class_counts = combined_dataset['class'].value_counts()
class_counts
# Plot bar chart
plt.figure(figsize=(6, 4))
plt.bar(class_counts.index.astype(str), class_counts.values, color=['blue', 'orange'])
plt.xlabel("Class")
plt.ylabel("Count")
plt.title("Distribution of Class Labels (0 vs 1)")
plt.xticks([0, 1], labels=['0', '1'])
plt.show()





# Separate majority and minority classes
majority = combined_dataset[combined_dataset['class'] == 0]
minority = combined_dataset[combined_dataset['class'] == 1]

# Desired number of majority samples (2x minority)
desired_majority_size = 7 * len(minority)

# Downsample majority class
majority_downsampled = resample(majority,
                                   replace=False,
                                   n_samples=desired_majority_size,
                                   random_state=42)

# Combine minority class with downsampled majority class
df_resampled = pd.concat([majority_downsampled, minority])

# Shuffle the result
combined_dataset = df_resampled.sample(frac=1, random_state=42).reset_index(drop=True)

class_counts = combined_dataset['class'].value_counts()
class_counts
# Plot bar chart
plt.figure(figsize=(6, 4))
plt.bar(class_counts.index.astype(str), class_counts.values, color=['blue', 'orange'])
plt.xlabel("Class")
plt.ylabel("Count")
plt.title("Distribution of Class Labels (0 vs 1)")
plt.xticks([0, 1], labels=['0', '1'])
plt.show()





# Compute the correlation matrix after dropping non-numeric or irrelevant columns
correlation_matrix = combined_dataset.drop(columns=['class', 'year']).corr()

# Set a threshold for identifying high correlations (ignoring perfect correlation of 1)
threshold = 0.8
highly_correlated = (correlation_matrix.abs() > threshold) & (correlation_matrix.abs() < 1)

# Initialize a list to store the pairs of highly correlated columns
correlated_pairs = []

# Iterate through the upper triangle of the correlation matrix to avoid duplicates
for i in range(len(correlation_matrix.columns)):
    for j in range(i + 1, len(correlation_matrix.columns)):
        if highly_correlated.iloc[i, j]:
            # Append the column names and their correlation value
            correlated_pairs.append((
                correlation_matrix.columns[i],
                correlation_matrix.columns[j],
                correlation_matrix.iloc[i, j]
            ))

# Print the pairs of columns that are highly correlated
print("Highly correlated columns (above threshold of 0.8):")
for col1, col2, corr in correlated_pairs:
    print(f"{col1} <--> {col2} (Correlation: {corr:.2f})")






# Create a heatmap to visualize the correlation matrix
sns.heatmap(correlation_matrix)

# Set the title of the heatmap plot
plt.title("Correlation Heatmap")

# Display the plot
plt.show()





# Initialize an empty set to store names of columns to drop
columns_to_drop = set()

# Iterate through the list of correlated column pairs
for col1, col2, corr in correlated_pairs:
    # If neither column has been marked for dropping yet
    if col1 not in columns_to_drop and col2 not in columns_to_drop:
        columns_to_drop.add(col1)  # Arbitrarily choose to drop col1 to reduce redundancy

# Drop the selected highly correlated columns from the dataset
reduced_dataset = combined_dataset.drop(columns=columns_to_drop)

# Also drop the 'year' column explicitly (assuming it's not needed)
reduced_dataset = reduced_dataset.drop(columns=['year'])

# Print how many columns were dropped due to high correlation
print(f"Dropped {len(columns_to_drop)} highly correlated columns.")

# Print the shape of the reduced dataset
print("Remaining dataset shape:", reduced_dataset.shape)





numerical_cols = reduced_dataset.select_dtypes(include=['int64', 'float64']).columns

num_cols = len(numerical_cols)
num_rows = (num_cols // 4) + 1  # Create a grid with 4 plots per row

plt.figure(figsize=(15, num_rows * 3))

for i, col in enumerate(numerical_cols, 1):
    plt.subplot(num_rows, 4, i)
    plt.hist(combined_dataset[col], bins=30, alpha=0.7, color='blue', edgecolor='black')
    plt.title(col)
    plt.xlabel("Value")
    plt.ylabel("Frequency")

plt.tight_layout()
plt.show()






correlation = combined_dataset.corr()['class'].sort_values(ascending=False)
print(correlation)





scaler = StandardScaler()

# Select numerical columns excluding 'class'
numerical_cols = reduced_dataset.select_dtypes(include=['int64', 'float64']).columns
numerical_cols = numerical_cols.drop('class', errors='ignore')  # Exclude class column
# Apply standardization
scaled_dataset = scaler.fit_transform(reduced_dataset[numerical_cols])
#dont run yet
print("All numerical features have been scaled using StandardScaler.")
scaled_dataset





scaled_dataset = pd.DataFrame(scaled_dataset, columns=numerical_cols)

# Re-add the class column
scaled_dataset['class'] = reduced_dataset['class'].values  # Ensure correct alignment

# Display a preview
scaled_dataset.head()





# Select numerical columns from the scaled dataset
numerical_cols = scaled_dataset.select_dtypes(include=['int64', 'float64']).columns

# Calculate the number of rows needed to plot 4 histograms per row
num_cols = len(numerical_cols)
num_rows = (num_cols // 4) + 1  # Ensures enough rows to fit all plots

# Set figure size based on the number of rows
plt.figure(figsize=(15, num_rows * 3))

# Loop through each numerical column and create a histogram
for i, col in enumerate(numerical_cols, 1):
    plt.subplot(num_rows, 4, i)  # Position the subplot in the grid
    plt.hist(scaled_dataset[col], bins=30, alpha=0.7, color='blue', edgecolor='black')
    plt.title(col)               # Set the title as the column name
    plt.xlabel("Value")          # X-axis label
    plt.ylabel("Frequency")      # Y-axis label

# Adjust spacing to prevent overlap between subplots
plt.tight_layout()
plt.show()






# near-zero variance
data_no_class = scaled_dataset.drop(columns=['class'], errors='ignore')
plt.figure(figsize=(8, 5))
sns.heatmap(data_no_class.var().to_frame().T, annot=False, cmap='coolwarm')
plt.title('Heatmap of predictor Variances')
plt.xlabel('Predictors')
plt.ylabel('Variance ')
plt.xticks(rotation=45, ha='right')
plt.show()





from sklearn.feature_selection import VarianceThreshold
selector = VarianceThreshold(threshold=0.01)

# Fit and transform the data
filtered_data = selector.fit_transform(data_no_class)

# Get the remaining feature names
remaining_features = data_no_class.columns[selector.get_support()]
removed_features = data_no_class.columns[~selector.get_support()]
print("Remaining Features:", remaining_features)
print("Removed Features:", removed_features)





from imblearn.over_sampling import SMOTE
from collections import Counter

# Split features and target variable
X = reduced_dataset.drop(columns=['class'])
y = reduced_dataset['class']

# Apply SMOTE
smote = SMOTE(sampling_strategy='auto', random_state=42)  # 'auto' balances to the majority class
X_resampled, y_resampled = smote.fit_resample(X, y)

# Convert back to DataFrame
balanced_df = pd.DataFrame(X_resampled, columns=X.columns)
balanced_df['class'] = y_resampled

# Check new class distribution
print("Class distribution after SMOTE:", Counter(y_resampled))









X = reduced_dataset.drop(columns=['class'])
y = reduced_dataset['class']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize StratifiedKFold
sKF = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

# Create the scaler and SMOTE
scaler = StandardScaler()
smote = SMOTE(random_state=42)

# Define classifiers and their hyperparameter grids
models = {
    'Logistic Regression': {
        'model': LogisticRegression(),
        'param_grid': {
        }
    },
    'SVM': {
        'model': SVC(),
        'param_grid': {
            'model__C': [1e-3, 1e-2, 1],
            'model__kernel': ['linear', 'rbf']
        }
    },
    'Decision Tree': {
        'model': DecisionTreeClassifier(),
        'param_grid': {
            'model__max_depth': [5, 10, 15, None],
            'model__min_samples_split': [2, 5, 10]
        }
    },
    'Random Forest': {
        'model': RandomForestClassifier(),
        'param_grid': {
            'model__n_estimators': [50, 100, 200],
            'model__max_depth': [3, 5, 10, None],
            'model__min_samples_split': [2, 5]
        }
    },
    'Gradient Boosting': {
        'model': GradientBoostingClassifier(),
        'param_grid': {
            'model__n_estimators': [50, 100, 200],
            'model__learning_rate': [1e-4, 1e-3, 1e-2],
            'model__max_depth': [3, 5, 7]
        }
    },
    'XGBoost': {
        'model': xgb.XGBClassifier(),
        'param_grid': {
            'model__n_estimators': [50, 100, 200],
            'model__learning_rate': [1e-4, 1e-3, 1e-2],
            'model__max_depth': [3, 5, 7]
        }
    }
}

# Store results
results = []

# Define the pipeline
pipeline = Pipeline([
    ('scaler', scaler),
    ('smote', smote),
    ('model', None)  # Placeholder for model
])

# Iterate over each model and perform GridSearchCV
for model_name, model_info in models.items():
    print(f"Training {model_name}...")

    # Set the model dynamically in the pipeline
    pipeline.set_params(model=model_info['model'])

    # Initialize GridSearchCV for hyperparameter tuning
    grid_search = GridSearchCV(
        estimator = pipeline,
        param_grid=model_info['param_grid'],
        # n_iter=3,
        cv=sKF,
        n_jobs=-1,
        verbose=1,
        # random_state=42,
        scoring='accuracy'
    )

    # Fit the grid search to find the best model
    grid_search.fit(X_train, y_train)

    # Get the best model and the corresponding hyperparameters
    best_model =  grid_search.best_estimator_
    best_params =  grid_search.best_params_

    print(f"Best parameters for {model_name}: {best_model}")
    print(f"Best score for {model_name}: {grid_search.best_score_}")


    # Initialize lists to store performance metrics for each fold
    fold_results = {
        'Precision': [],
        'Recall': [],
        'F1-Score': [],
        'Accuracy': [],
        'Kappa': []
    }

    # Perform cross-validation with the best model
    for train_idx, test_idx in sKF.split(X_train, y_train):
        X_train_cv, X_test_cv = X_train.iloc[train_idx], X_train.iloc[test_idx]
        y_train_cv, y_test_cv = y_train.iloc[train_idx], y_train.iloc[test_idx]

        # Fit the best model
        best_model.fit(X_train_cv, y_train_cv)

        # Predict and evaluate the model
        y_pred = best_model.predict(X_test_cv)

        # Calculate precision, recall, F1-score, and accuracy for each fold
        precision, recall, f1, _ = precision_recall_fscore_support(y_test_cv, y_pred, average='binary')
        accuracy = accuracy_score(y_test_cv, y_pred)
        # Kappa
        kappa = cohen_kappa_score(y_test_cv, y_pred)

        # Append metrics for the current fold
        fold_results['Precision'].append(precision)
        fold_results['Recall'].append(recall)
        fold_results['F1-Score'].append(f1)
        fold_results['Accuracy'].append(accuracy)
        fold_results['Kappa'].append(kappa)

    # Calculate the mean of each metric across all folds
    mean_results = {
        'Model': model_name,
        'Best Params': str(best_params),
        'Precision': np.mean(fold_results['Precision']),
        'Recall': np.mean(fold_results['Recall']),
        'F1-Score': np.mean(fold_results['F1-Score']),
        'Accuracy': np.mean(fold_results['Accuracy']),
        'Kappa': np.mean(fold_results['Kappa'])
    }

    # Append the mean results to the final results list
    results.append(mean_results)

# Convert results to a DataFrame for better readability
results_df = pd.DataFrame(results)
print(results_df)





# test models
test_results = []

for model_name, model_info in models.items():
    print(f"Testing {model_name} on test set...")

    # Get the best model for the current model_name
    best_model = results_df.loc[results_df['Model'] == model_name, 'Best Params'].values[0]

    # Convert string representation of parameters back to a dictionary
    best_params = eval(best_model)

    # Set the best model with best parameters
    pipeline.set_params(model=model_info['model'])
    pipeline.set_params(**best_params)

    # Fit the best model on the full training set
    pipeline.fit(X_train, y_train)

    # Make predictions on the test set
    y_pred_test = pipeline.predict(X_test)

    # Evaluate performance on the test set
    precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred_test, average='binary')
    accuracy = accuracy_score(y_test, y_pred_test)
     # Kappa
    kappa = cohen_kappa_score(y_test, y_pred_test)

    # Store test results
    test_results.append({
        'Model': model_name,
        'Precision (Test)': precision,
        'Recall (Test)': recall,
        'F1-Score (Test)': f1,
        'Accuracy (Test)': accuracy,
        'Kappa (Test)': kappa

    })

# Convert test results to a DataFrame
test_results_df = pd.DataFrame(test_results)
print("\nTest Set Results:")
print(test_results_df)





X = reduced_dataset.drop(columns=['class'])
y = reduced_dataset['class']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)


# Create the scaler
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)
# smote
X_train_smote, y_train_smote = smote.fit_resample(X_train_scaled, y_train)

print(X_train_scaled.shape)
print(y_train.shape)
print(X_test_scaled.shape)
print(y_test.shape)





# existing code for model building with variable layers and dropout rate
def call_existing_code(units_per_layer, dropout_rate,lr):
    model = Sequential()
    # tune number of units separately
    for units in units_per_layer:
        # model.add(layers.Dense(units=units, activation='relu',kernel_regularizer=regularizers.l2(l2_rate)))
        model.add(layers.Dense(units=units, activation='relu'))
        # tune whether to use dropout
        if dropout_rate:
            model.add(layers.Dropout(rate=dropout_rate))

    model.add(layers.Dense(1, activation="sigmoid"))
    model.compile(
        optimizer=keras.optimizers.Adam(learning_rate=lr),
        loss='binary_crossentropy',
        metrics=["accuracy"],
    )
    return model

# Define the model builder function for the tuner
def build_model(hp):
    input_shape = (X_train_scaled.shape[1],)
    num_layers = hp.Int("num_layers", min_value=1, max_value=5)
    units_per_layer = [hp.Choice(f"units_layer{i}", [64,128,256]) for i in range(num_layers)]
    # activation = hp.Choice("activation", ["relu", "tanh"])
    dropout_rate = hp.Choice("dropout_rate", [0.2, 0.3])
    # dropout_rate = hp.Float("dropout_rate", min_value=0.1, max_value=0.5, step=0.05)
    lr = hp.Float("lr", min_value=1e-5, max_value=1e-3, sampling="log")
    # l2_rate = hp.Float("l2_rate", min_value=1e-7, max_value=1e-3, sampling="log")

    # Call existing model-building code with the hyperparameter values
    # model = call_existing_code(units_per_layer=units_per_layer, dropout_rate=dropout_rate, lr=lr, l2_rate=l2_rate)
    model = call_existing_code(units_per_layer=units_per_layer, dropout_rate=dropout_rate, lr=lr)

    return model

# Example usage with keras-tuner
tuner = kt.RandomSearch(
    build_model,
    objective='val_accuracy',
    max_trials=5,
    executions_per_trial=3,
    overwrite=True,
    directory='/content/drive/Shareddrives/CS5831 Data Mining Spring 2025',
    project_name='Test'
)

early_stopping = EarlyStopping(monitor='val_accuracy', patience=12, mode='max')

tuner.search(X_train_smote, y_train_smote, epochs=200, validation_split=0.2, callbacks=[early_stopping])
# best_model = tuner.get_best_models(num_models=1)[0]

# Get the top 2 models.
models = tuner.get_best_models(num_models=2)
best_model = models[0]
best_model.build(input_shape=(None, X_train_scaled.shape[1]))
best_model.summary()
# To get the best hyperparameters
best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]

# plot(history)
# the optimal l2 rate is {best_hps.get('l2_rate')},

print(f"""
The hyperparameter search is complete. The optimal number of layers is {best_hps.get('num_layers')},
whether to use dropout is {best_hps.get('dropout_rate')},
and the optimal learning rate for the optimizer is {best_hps.get('lr')}.
""")





# ANN
def ANN():
    M1_input = Input(shape=(X_train_scaled.shape[1],))
    M1_dense_1 = Dense(256)(M1_input)
    drop_out = Dropout(0.3)(M1_dense_1)

    M1_dense_2 = Dense(256, activation='relu')(drop_out)
    drop_out = Dropout(0.3)(M1_dense_2)

    M1_dense_3 = Dense(64, activation='relu')(drop_out)
    drop_out = Dropout(0.3)(M1_dense_3)

    M1_dense_4 = Dense(64, activation='relu')(drop_out)
    drop_out = Dropout(0.3)(M1_dense_4)

    # M1_dense_5 = Dense(128, activation='relu')(drop_out)
    # drop_out = Dropout(0.3)(M1_dense_5)

    M1_output = Dense(1, activation='sigmoid')(drop_out)

    model = Model(inputs=M1_input, outputs=M1_output)

    return model





def plot(history):
    # learning curves of model accuracy
    plt.plot(history.history['accuracy'], label='train_acc')
    plt.plot(history.history['val_accuracy'], label='val_acc')
    plt.plot(history.history['loss'], label='train_loss')
    plt.plot(history.history['val_loss'], label='val_loss')
    plt.legend()
    plt.show()





# Initialize KFold
sKF = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

fold = 1
ann_scores=[]
for train, val in sKF.split(X_train_scaled,y_train):

    X_train_pt, X_val_pt = X_train_scaled[train], X_train_scaled[val]
    y_train_pt, y_val = y_train.iloc[train], y_train.iloc[val]
    # Apply SMOTE to the training data only
    X_train_smote, y_train_smote = smote.fit_resample(X_train_pt, y_train_pt)
    # Compute class weights for the resampled data
    class_weights = compute_class_weight(
        class_weight='balanced',
        classes=np.unique(y_train_smote),
        y=y_train_smote
    )
    class_weight_dict = dict(zip(np.unique(y_train_smote), class_weights))

    model = ANN()
    model.compile(optimizer=Adam(learning_rate=5e-05 ),
              loss=BinaryCrossentropy(),
              metrics=['accuracy'])

    metric = 'val_accuracy'

    es = EarlyStopping(monitor='val_accuracy', patience=10, mode='max')

    # Training and Evaluation
    history = model.fit(X_train_scaled, y_train, epochs=80, verbose=1,
                        class_weight=class_weight_dict,
                        validation_data = (X_val_pt, y_val), callbacks=[es])

   # After predicting probabilities
    y_pred_prob = model.predict(X_val_pt).reshape(-1)

    # === Find best threshold ===
    precision_vals, recall_vals, thresholds = precision_recall_curve(y_val, y_pred_prob)
    f1_scores = 2 * (precision_vals * recall_vals) / (precision_vals + recall_vals + 1e-8)
    best_idx = np.argmax(f1_scores)
    best_threshold = thresholds[best_idx]

    print(f"Best threshold for F1: {best_threshold:.2f}")

    # Predict using the optimized threshold
    y_pred = (y_pred_prob >= best_threshold).astype(int)
    y_val = np.array(y_val)
    y_pred = np.array(y_pred)

    # calculate metrics
    kappa = cohen_kappa_score(y_val, y_pred)
    accuracy = accuracy_score(y_val,y_pred)
    precision = precision_score(y_val,y_pred)
    recall = recall_score(y_val,y_pred)
    f1 = f1_score(y_val,y_pred)
    mcc = matthews_corrcoef(y_val,y_pred)
    cm = confusion_matrix (y_val,y_pred)
    tn, fp, fn, tp = confusion_matrix(y_val, y_pred).ravel()
    specificity = tn / (tn+fp)
    # binary classification roc
    auroc = roc_auc_score(y_val,y_pred_prob)
    aupr = average_precision_score(y_val,y_pred_prob)
    # append results
    ann_scores.append((fold,recall,specificity,accuracy,precision,f1,auroc,aupr,mcc))
    plot(history)
    #
    # Print the metrics in a readable format
    print(f"Kappa  : {kappa:.2f}")
    print(f"Accuracy  : {accuracy:.2f}")
    print(f"Precision : {precision:.2f}")
    print(f"Recall    : {recall:.2f}")
    print(f"F1 Score  : {f1:.2f}")
    print(f"MCC       : {mcc:.2f}")
    print(f"Specificity: {specificity:.2f}")
    print(f"AUROC     : {auroc:.2f}")
    print(f"AUPR      : {aupr:.2f}")

    # Print confusion matrix
    print("\nConfusion Matrix:")
    print(f"              Predicted No     Predicted Yes")
    print(f"Actual No     {cm[0][0]:>6}            {cm[0][1]:>6}")
    print(f"Actual Yes    {cm[1][0]:>6}            {cm[1][1]:>6}")
    # print("\n %s, %s, %s, %s, %s \n" %(str(acc), str(mcc), str(sn), str(sp), cm))
    fold += 1

cols = ['Fold','Sensitivity', 'Specificity','Accuracy','Precision','F1 Score','AUC-ROC','AUC-PR','MCC']
df_ann = pd.DataFrame(ann_scores, columns=cols)





# train on the whole dataset
model = ANN()
model.compile(optimizer=Adam(learning_rate=5e-5),
          loss=BinaryCrossentropy(),
          metrics=['accuracy'])

metric = 'accuracy'

es = EarlyStopping(monitor='accuracy', patience=8, mode='max')

# Training and Evaluation
model.fit(X_train_smote, y_train_smote, batch_size=32, epochs=70, verbose=1, callbacks=[es])





# test on testset
y_pred_prob = model.predict(X_test_scaled).reshape(-1)

# === Find best threshold ===
precision_vals, recall_vals, thresholds = precision_recall_curve(y_test, y_pred_prob)
f1_scores = 2 * (precision_vals * recall_vals) / (precision_vals + recall_vals + 1e-8)
best_idx = np.argmax(f1_scores)
best_threshold = thresholds[best_idx]
# Predict using the optimized threshold
y_pred = (y_pred_prob >= best_threshold).astype(int)

print(f"Best threshold for F1: {best_threshold:.2f}")

# Evaluate the best model
kappa = cohen_kappa_score(y_test, y_pred)
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
mcc = matthews_corrcoef(y_test, y_pred)
cm = confusion_matrix (y_test, y_pred)
tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()
specificity = tn / (tn+fp)
auroc = roc_auc_score(y_test, y_pred_prob)
aupr = average_precision_score(y_test, y_pred_prob)

# Print the metrics in a readable format
print(f"Kappa  : {kappa:.2f}")
print(f"Accuracy  : {accuracy:.2f}")
print(f"Precision : {precision:.2f}")
print(f"Recall    : {recall:.2f}")
print(f"F1 Score  : {f1:.2f}")
print(f"MCC       : {mcc:.2f}")
print(f"Specificity: {specificity:.2f}")
print(f"AUROC     : {auroc:.2f}")
print(f"AUPR      : {aupr:.2f}")

# Print confusion matrix
print("\nConfusion Matrix:")
print(f"              Predicted No     Predicted Yes")
print(f"Actual No     {cm[0][0]:>6}            {cm[0][1]:>6}")
print(f"Actual Yes    {cm[1][0]:>6}            {cm[1][1]:>6}")

# store metrics for data frmae
ann_scores=[]
ann_scores.append((recall,specificity,kappa,accuracy,precision,f1,auroc,aupr,mcc))
# create dataframe
cols = ['Sensitivity', 'Specificity','Kappa','Accuracy','Precision','F1 Score','AUC-ROC','AUC-PR','MCC']
df_ann = pd.DataFrame(ann_scores, columns=cols)
df_ann
